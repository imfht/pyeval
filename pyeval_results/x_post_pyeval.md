# 当所有大模型都能考 90 分，怎么找出真正的差距？

我们搭了一个 Python 编程能力评测框架（PyEval），拿 8 个主流模型跑了一圈，发现一个尴尬的问题：

**标准模式下，模型得分挤在 91%-100% 之间，几乎没有区分度。**

于是我们新增了两种评测模式，结果很有意思。分享一下。

---

## 背景：标准模式的天花板

PyEval 有 45 道 Python 编程题（easy/medium/hard），覆盖算法、OOP、并发等 10 个方向。标准模式就是给函数签名 + docstring，让模型写实现。

结果：

| 模型 | 标准模式通过率 |
|------|-------------|
| qwen3.5-plus | 100% |
| qwen3.5-flash | 100% |
| glm-5 | 97.8% |
| qwen3-max | 95.6% |
| deepseek-v3.2 | 95.6% |
| kimi-k2.5 | 93.3% |
| glm-4.7 | 93.3% |
| MiniMax-M2.5 | 91.1% |

八个模型挤在 91-100%，这能说明什么？说明不了太多。

---

## 新模式一：Bug 修复

给模型一段有 bug 的代码 + bug 描述，让它修。15 道题覆盖经典 Python 陷阱：

- Easy：off-by-one、比较符写反、缺 return、变量拼错、条件反了
- Medium：浅拷贝、可变默认参数、错误异常类型、闭包捕获、整除 vs 浮点除
- Hard：线程竞态、生成器耗尽、装饰器丢 wraps、钻石继承 MRO、async 上下文管理器

**结果：**

| 模型 | Bugfix 通过率 |
|------|-------------|
| qwen3.5-plus | 15/15 (100%) |
| qwen3.5-flash | 15/15 (100%) |
| qwen3-max | 15/15 (100%) |
| deepseek-v3.2 | 15/15 (100%) |
| kimi-k2.5 | 15/15 (100%) |
| glm-5 | 14/15 (93.3%) |
| glm-4.7 | 14/15 (93.3%) |
| MiniMax-M2.5 | 13/15 (86.7%) |

头部五个模型全满分——Bug 修复题对强模型来说确实不难，bug 位置和描述都给了，相当于开卷考试。但尾部开始拉开差距。

---

## 新模式二：多轮纠错（核心发现）

这才是真正拉开差距的模式。

**规则：**
- 第 1 次：标准调用，写对了算 1.0 分
- 第 1 次失败 → 把报错信息反馈给模型重试，写对了算 0.6 分
- 第 2 次还失败 → 再反馈再试，写对了算 0.3 分
- 3 次都没过 → 0 分

加权分 = Σ(尝试权重 × 难度权重) / Σ(难度权重)

这模拟的是真实开发场景：写完跑一下，报错了改，再跑，再改。

**结果：**

| 模型 | 首次通过 | 重试后通过 | 加权分 | 需纠错题数 |
|------|---------|-----------|--------|-----------|
| qwen3.5-plus | 97.8% | 100% | **99.1%** | 1 题 (att.2) |
| qwen3-max | 97.8% | 100% | **99.1%** | 1 题 (att.2) |
| kimi-k2.5 | 97.8% | 100% | **99.1%** | 1 题 (att.2) |
| qwen3.5-flash | 95.6% | 100% | **98.1%** | 2 题 (att.2) |
| deepseek-v3.2 | 95.6% | 100% | **98.1%** | 2 题 (att.2) |
| glm-5 | 95.6% | 97.8% | **96.3%** | 1 题 (att.2), 1 题失败 |
| MiniMax-M2.5 | 88.9% | 100% | **94.9%** | 5 题 (att.2) |
| glm-4.7 | 80.0% | 93.3% | **86.0%** | 6 题 (att.2), 3 题失败 |

---

## 八模型全景对比

| 模型 | 标准模式 | Bugfix | 多轮加权 | 综合排名 |
|------|---------|--------|---------|---------|
| qwen3.5-plus | 100% | 100% | 99.1% | 1 |
| qwen3-max | 95.6% | 100% | 99.1% | 2 |
| kimi-k2.5 | 93.3% | 100% | 99.1% | 3 |
| qwen3.5-flash | 100% | 100% | 98.1% | 4 |
| deepseek-v3.2 | 95.6% | 100% | 98.1% | 5 |
| glm-5 | 97.8% | 93.3% | 96.3% | 6 |
| MiniMax-M2.5 | 91.1% | 86.7% | 94.9% | 7 |
| glm-4.7 | 93.3% | 93.3% | 86.0% | 8 |

---

## 区分度对比：从 9pp 到 13pp

**标准模式区间：91.1% - 100%（差 9pp）**
**多轮加权区间：86.0% - 99.1%（差 13pp）**

更重要的是，多轮模式改变了排名结构。标准模式下，八个模型挤成一团；多轮模式下，清晰分出了三个梯队：
- **第一梯队（99%+）**：qwen3.5-plus、qwen3-max、kimi-k2.5
- **第二梯队（94-98%）**：qwen3.5-flash、deepseek-v3.2、glm-5、MiniMax-M2.5
- **第三梯队（86%）**：glm-4.7

---

## 几个有趣的发现

**1. 第一梯队形成"三强"格局**

qwen3.5-plus、qwen3-max、kimi-k2.5 三个模型在多轮模式下都达到了 99.1% 的加权分，只各有 1 道题需要第 2 次尝试。它们的纠错能力几乎完美——给个报错就能立刻改对。

**2. kimi-k2.5 是隐藏的强者**

标准模式 93.3% 看起来一般，但多轮模式 99.1% 与 qwen3.5-plus 并列。这说明 kimi 的"一次性写对率"稍低，但纠错能力极强——给个报错信息就能立刻改对。在真实开发中，这种模型其实很好用。

**3. deepseek-v3.2 表现稳健**

标准模式 95.6%，Bugfix 满分，多轮 98.1%——在所有维度上都非常扎实。虽然没进第一梯队（差 1pp），但它是唯一一个在 Bugfix + 多轮都拿到高分的非 Qwen 系模型（kimi 除外）。

**4. qwen3.5-flash：用 flash 的价格买到 plus 九成功力**

这可能是本次评测中性价比最高的模型。作为轻量级/快速推理版本，qwen3.5-flash 的成绩令人意外：

| | 标准 | Bugfix | 多轮 | 定位 |
|---|---|---|---|---|
| qwen3.5-plus | 100% | 100% | 99.1% | 旗舰 |
| **qwen3.5-flash** | **100%** | **100%** | **98.1%** | **轻量** |
| qwen3-max | 95.6% | 100% | 99.1% | 旗舰 |
| deepseek-v3.2 | 95.6% | 100% | 98.1% | 旗舰 |
| kimi-k2.5 | 93.3% | 100% | 99.1% | 旗舰 |

几个关键事实：

- **标准模式 100%，打赢了 qwen3-max、deepseek-v3.2、kimi-k2.5 三个旗舰模型**。一个轻量模型在"一次性写对"能力上排名第一梯队，这很少见。
- Bugfix 满分，15 道 Python 陷阱题全部修对，包括线程竞态、钻石继承 MRO 等 hard 级别问题。
- 多轮模式只有 2 道题需要第 2 次尝试，0 道需要第 3 次。纠错风格非常干脆——不是"大概改改看看"，而是一次改对。
- 与第一梯队的差距仅 1pp（98.1% vs 99.1%），而推理成本可能只有旗舰模型的几分之一。

如果你的场景是日常 Python 编程辅助，qwen3.5-flash 可能是最务实的选择：它几乎能做到旗舰模型能做的一切，但更快更便宜。唯一的短板在少数 hard 级别 OOP/异常处理题上稳定性略低——但多轮模式下也都能自行修正。

**5. glm-5：被 API 稳定性耽误的实力派**

glm-5 的故事值得单独说。首轮评测中，它标准模式仅 68.9%、多轮仅 62.2%，看起来远远落后。但大量失败是 API 超时和 500 错误导致的。

我们将 API 超时从 60s 提升到 120s 后重跑，结果翻天覆地：

| 模式 | 首轮 | 重跑 |
|------|------|------|
| 标准 | 68.9% | **97.8%** |
| Bugfix | 66.7% | **93.3%** |
| 多轮加权 | 62.2% | **96.3%** |

97.8% 的标准通过率甚至超过了 qwen3-max 和 deepseek-v3.2。这提醒我们：**评测结果高度依赖 API 稳定性**，一个超时配置的差异就能让模型从末位变成中上游。

**6. MiniMax-M2.5：纠错能力的逆袭**

MiniMax-M2.5 的多轮成绩也大幅提升——从 82.3% 升至 94.9%。标准模式首次成功跑出 91.1%。它的特点是：
- 标准模式和 Bugfix 得分在 8 个模型中垫底
- 但多轮纠错能力很强：5 道题靠第 2 次尝试修正，最终 100% 通过
- 说明它"第一遍写个大概，靠报错逐步修正"的风格依然明显，但修正效率比之前观察到的要好得多

**7. glm-4.7 成为唯一掉队者**

重跑后排名变化最大的发现：glm-4.7 从中游掉到末位。之前 glm-5 和 MiniMax-M2.5 因为 API 问题垫底，掩盖了 glm-4.7 的真实位置。现在看来，glm-4.7 的多轮加权 86.0%（3 道题彻底失败）是八个模型中唯一低于 90% 的，存在明显的纠错能力短板。

**8. "写对"和"改对"是两种不同能力**

标准模式只测"一次性写对"，但真实开发中，快速定位并修复错误的能力同样重要。多轮模式同时衡量了这两者，而且通过加权机制，一次写对的价值远高于多轮修改（1.0 vs 0.6 vs 0.3）。

---

## 评测方法

- 框架：PyEval（自研，纯 Python，无外部依赖）
- 标准题库：45 题 × 10 类别 × 3 难度
- Bugfix 题库：15 题 × 3 难度，覆盖经典 Python 陷阱
- 多轮纠错：最多 3 次尝试，递减加权（1.0 / 0.6 / 0.3）
- 所有模型通过阿里云百炼 DashScope API 调用，temperature=0
- 代码在隔离子进程中执行，通过 unittest 验证正确性
- 代码开源：[github.com/imfht/pyeval](https://github.com/imfht/pyeval)

---

*如果你也在做模型评测，不妨试试多轮纠错模式——它能把 90 分的天花板撬开，看到更真实的能力分布。*
