[
  {
    "id": "fio_001",
    "category": "file_io",
    "difficulty": "easy",
    "title": "Read CSV Data",
    "prompt": "import csv\nimport io\n\ndef read_csv_data(csv_string: str) -> list:\n    \"\"\"\n    Parse a CSV-formatted string into a list of dictionaries.\n\n    The first row of the CSV string contains the headers, which become\n    the dictionary keys. Each subsequent row becomes a dictionary with\n    values mapped to the corresponding headers.\n\n    Use the csv module for parsing. Handle quoted fields correctly.\n\n    Args:\n        csv_string: A string containing CSV-formatted data\n\n    Returns:\n        A list of dictionaries, one per data row\n\n    Examples:\n        read_csv_data('name,age\\nAlice,30\\nBob,25')\n        -> [{'name': 'Alice', 'age': '30'}, {'name': 'Bob', 'age': '25'}]\n\n        read_csv_data('a,b\\n1,2')\n        -> [{'a': '1', 'b': '2'}]\n    \"\"\"",
    "reference_solution": "import csv\nimport io\n\ndef read_csv_data(csv_string: str) -> list:\n    \"\"\"\n    Parse a CSV-formatted string into a list of dictionaries.\n\n    The first row of the CSV string contains the headers, which become\n    the dictionary keys. Each subsequent row becomes a dictionary with\n    values mapped to the corresponding headers.\n\n    Use the csv module for parsing. Handle quoted fields correctly.\n\n    Args:\n        csv_string: A string containing CSV-formatted data\n\n    Returns:\n        A list of dictionaries, one per data row\n\n    Examples:\n        read_csv_data('name,age\\nAlice,30\\nBob,25')\n        -> [{'name': 'Alice', 'age': '30'}, {'name': 'Bob', 'age': '25'}]\n\n        read_csv_data('a,b\\n1,2')\n        -> [{'a': '1', 'b': '2'}]\n    \"\"\"\n    reader = csv.DictReader(io.StringIO(csv_string))\n    return [dict(row) for row in reader]",
    "test_code": "import unittest\n\nclass TestReadCsvData(unittest.TestCase):\n    def test_basic(self):\n        result = read_csv_data('name,age\\nAlice,30\\nBob,25')\n        self.assertEqual(result, [{'name': 'Alice', 'age': '30'}, {'name': 'Bob', 'age': '25'}])\n\n    def test_single_column(self):\n        result = read_csv_data('x\\n1\\n2\\n3')\n        self.assertEqual(result, [{'x': '1'}, {'x': '2'}, {'x': '3'}])\n\n    def test_quoted_fields(self):\n        result = read_csv_data('name,city\\n\"Doe, John\",\"New York\"')\n        self.assertEqual(result, [{'name': 'Doe, John', 'city': 'New York'}])\n\n    def test_empty_data(self):\n        result = read_csv_data('a,b')\n        self.assertEqual(result, [])\n\n    def test_single_row(self):\n        result = read_csv_data('a,b\\n1,2')\n        self.assertEqual(result, [{'a': '1', 'b': '2'}])\n\n    def test_whitespace_values(self):\n        result = read_csv_data('k,v\\n hello , world ')\n        self.assertEqual(result, [{'k': ' hello ', 'v': ' world '}])\n\n    def test_many_columns(self):\n        result = read_csv_data('a,b,c,d\\n1,2,3,4')\n        self.assertEqual(len(result), 1)\n        self.assertEqual(len(result[0]), 4)\n        self.assertEqual(result[0], {'a': '1', 'b': '2', 'c': '3', 'd': '4'})\n\n    def test_multiple_rows(self):\n        csv_str = 'id,name,score\\n1,Alice,95\\n2,Bob,87\\n3,Charlie,92'\n        result = read_csv_data(csv_str)\n        self.assertEqual(len(result), 3)\n        self.assertEqual(result[0]['name'], 'Alice')\n        self.assertEqual(result[2]['score'], '92')",
    "tags": ["csv", "parsing", "io", "string-processing"],
    "time_limit_seconds": 5
  },
  {
    "id": "fio_002",
    "category": "file_io",
    "difficulty": "medium",
    "title": "Merge JSON Files",
    "prompt": "import json\n\ndef merge_json_files(json_strings: list) -> dict:\n    \"\"\"\n    Merge a list of JSON strings into a single dictionary.\n\n    Each JSON string represents a dictionary. Merge them left to right so\n    that later values override earlier ones for the same keys. For nested\n    dictionaries, perform a deep merge: if both the existing and new values\n    for a key are dicts, merge them recursively instead of replacing.\n\n    Non-dict values always override (even if the existing value is a dict).\n\n    Args:\n        json_strings: A list of strings, each a valid JSON object\n\n    Returns:\n        A single merged dictionary\n\n    Examples:\n        merge_json_files(['{\"a\": 1}', '{\"b\": 2}'])\n        -> {'a': 1, 'b': 2}\n\n        merge_json_files(['{\"a\": 1}', '{\"a\": 2}'])\n        -> {'a': 2}\n\n        merge_json_files(['{\"x\": {\"a\": 1, \"b\": 2}}', '{\"x\": {\"b\": 3, \"c\": 4}}'])\n        -> {'x': {'a': 1, 'b': 3, 'c': 4}}\n    \"\"\"",
    "reference_solution": "import json\n\ndef merge_json_files(json_strings: list) -> dict:\n    \"\"\"\n    Merge a list of JSON strings into a single dictionary.\n\n    Each JSON string represents a dictionary. Merge them left to right so\n    that later values override earlier ones for the same keys. For nested\n    dictionaries, perform a deep merge: if both the existing and new values\n    for a key are dicts, merge them recursively instead of replacing.\n\n    Non-dict values always override (even if the existing value is a dict).\n\n    Args:\n        json_strings: A list of strings, each a valid JSON object\n\n    Returns:\n        A single merged dictionary\n\n    Examples:\n        merge_json_files(['{\"a\": 1}', '{\"b\": 2}'])\n        -> {'a': 1, 'b': 2}\n\n        merge_json_files(['{\"a\": 1}', '{\"a\": 2}'])\n        -> {'a': 2}\n\n        merge_json_files(['{\"x\": {\"a\": 1, \"b\": 2}}', '{\"x\": {\"b\": 3, \"c\": 4}}'])\n        -> {'x': {'a': 1, 'b': 3, 'c': 4}}\n    \"\"\"\n    def deep_merge(base, override):\n        result = base.copy()\n        for key, value in override.items():\n            if key in result and isinstance(result[key], dict) and isinstance(value, dict):\n                result[key] = deep_merge(result[key], value)\n            else:\n                result[key] = value\n        return result\n\n    merged = {}\n    for s in json_strings:\n        data = json.loads(s)\n        merged = deep_merge(merged, data)\n    return merged",
    "test_code": "import unittest\nimport json\n\nclass TestMergeJsonFiles(unittest.TestCase):\n    def test_disjoint_keys(self):\n        result = merge_json_files(['{\"a\": 1}', '{\"b\": 2}'])\n        self.assertEqual(result, {'a': 1, 'b': 2})\n\n    def test_override_simple(self):\n        result = merge_json_files(['{\"a\": 1}', '{\"a\": 2}'])\n        self.assertEqual(result, {'a': 2})\n\n    def test_nested_merge(self):\n        result = merge_json_files(['{\"x\": {\"a\": 1, \"b\": 2}}', '{\"x\": {\"b\": 3, \"c\": 4}}'])\n        self.assertEqual(result, {'x': {'a': 1, 'b': 3, 'c': 4}})\n\n    def test_empty_list(self):\n        result = merge_json_files([])\n        self.assertEqual(result, {})\n\n    def test_single_item(self):\n        result = merge_json_files(['{\"key\": \"value\"}'])\n        self.assertEqual(result, {'key': 'value'})\n\n    def test_deep_nested_merge(self):\n        s1 = json.dumps({'a': {'b': {'c': 1, 'd': 2}}})\n        s2 = json.dumps({'a': {'b': {'d': 3, 'e': 4}}})\n        result = merge_json_files([s1, s2])\n        self.assertEqual(result, {'a': {'b': {'c': 1, 'd': 3, 'e': 4}}})\n\n    def test_override_dict_with_scalar(self):\n        s1 = json.dumps({'a': {'nested': True}})\n        s2 = json.dumps({'a': 42})\n        result = merge_json_files([s1, s2])\n        self.assertEqual(result, {'a': 42})\n\n    def test_override_scalar_with_dict(self):\n        s1 = json.dumps({'a': 1})\n        s2 = json.dumps({'a': {'b': 2}})\n        result = merge_json_files([s1, s2])\n        self.assertEqual(result, {'a': {'b': 2}})\n\n    def test_three_way_merge(self):\n        s1 = json.dumps({'a': 1, 'b': 2})\n        s2 = json.dumps({'b': 3, 'c': 4})\n        s3 = json.dumps({'c': 5, 'd': 6})\n        result = merge_json_files([s1, s2, s3])\n        self.assertEqual(result, {'a': 1, 'b': 3, 'c': 5, 'd': 6})\n\n    def test_list_values_override(self):\n        s1 = json.dumps({'items': [1, 2, 3]})\n        s2 = json.dumps({'items': [4, 5]})\n        result = merge_json_files([s1, s2])\n        self.assertEqual(result, {'items': [4, 5]})\n\n    def test_mixed_nested_and_flat(self):\n        s1 = json.dumps({'config': {'debug': True, 'db': {'host': 'localhost', 'port': 5432}}})\n        s2 = json.dumps({'config': {'db': {'port': 3306, 'name': 'mydb'}, 'verbose': False}})\n        result = merge_json_files([s1, s2])\n        expected = {'config': {'debug': True, 'db': {'host': 'localhost', 'port': 3306, 'name': 'mydb'}, 'verbose': False}}\n        self.assertEqual(result, expected)",
    "tags": ["json", "deep-merge", "recursion", "dictionary"],
    "time_limit_seconds": 5
  },
  {
    "id": "fio_003",
    "category": "file_io",
    "difficulty": "hard",
    "title": "Log Parser",
    "prompt": "import re\n\ndef log_parser(log_lines: list) -> dict:\n    \"\"\"\n    Parse a list of log line strings and extract structured information.\n\n    Each log line is in the format:\n        [TIMESTAMP] LEVEL: message\n    where TIMESTAMP is in 'YYYY-MM-DD HH:MM:SS' format and LEVEL is one\n    of DEBUG, INFO, WARNING, ERROR, CRITICAL.\n\n    Returns a dictionary with:\n        - 'counts': dict mapping each log level to its count\n        - 'errors': list of message strings from ERROR and CRITICAL lines\n          (in order of appearance)\n        - 'timestamp_range': tuple of (earliest, latest) timestamp strings,\n          or None if no log lines are provided\n\n    Args:\n        log_lines: List of log line strings\n\n    Returns:\n        Dict with 'counts', 'errors', and 'timestamp_range' keys\n\n    Examples:\n        log_parser(['[2024-01-15 10:30:45] ERROR: Connection failed',\n                     '[2024-01-15 10:31:00] INFO: Retrying'])\n        -> {\n            'counts': {'ERROR': 1, 'INFO': 1},\n            'errors': ['Connection failed'],\n            'timestamp_range': ('2024-01-15 10:30:45', '2024-01-15 10:31:00')\n        }\n    \"\"\"",
    "reference_solution": "import re\n\ndef log_parser(log_lines: list) -> dict:\n    \"\"\"\n    Parse a list of log line strings and extract structured information.\n\n    Each log line is in the format:\n        [TIMESTAMP] LEVEL: message\n    where TIMESTAMP is in 'YYYY-MM-DD HH:MM:SS' format and LEVEL is one\n    of DEBUG, INFO, WARNING, ERROR, CRITICAL.\n\n    Returns a dictionary with:\n        - 'counts': dict mapping each log level to its count\n        - 'errors': list of message strings from ERROR and CRITICAL lines\n          (in order of appearance)\n        - 'timestamp_range': tuple of (earliest, latest) timestamp strings,\n          or None if no log lines are provided\n\n    Args:\n        log_lines: List of log line strings\n\n    Returns:\n        Dict with 'counts', 'errors', and 'timestamp_range' keys\n\n    Examples:\n        log_parser(['[2024-01-15 10:30:45] ERROR: Connection failed',\n                     '[2024-01-15 10:31:00] INFO: Retrying'])\n        -> {\n            'counts': {'ERROR': 1, 'INFO': 1},\n            'errors': ['Connection failed'],\n            'timestamp_range': ('2024-01-15 10:30:45', '2024-01-15 10:31:00')\n        }\n    \"\"\"\n    counts = {}\n    errors = []\n    timestamps = []\n    pattern = re.compile(r'^\\[(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\\] (\\w+): (.*)$')\n\n    for line in log_lines:\n        match = pattern.match(line)\n        if match:\n            timestamp, level, message = match.groups()\n            counts[level] = counts.get(level, 0) + 1\n            if level in ('ERROR', 'CRITICAL'):\n                errors.append(message)\n            timestamps.append(timestamp)\n\n    if timestamps:\n        timestamps.sort()\n        timestamp_range = (timestamps[0], timestamps[-1])\n    else:\n        timestamp_range = None\n\n    return {\n        'counts': counts,\n        'errors': errors,\n        'timestamp_range': timestamp_range\n    }",
    "test_code": "import unittest\n\nclass TestLogParser(unittest.TestCase):\n    def test_basic(self):\n        lines = [\n            '[2024-01-15 10:30:45] ERROR: Connection failed',\n            '[2024-01-15 10:31:00] INFO: Retrying'\n        ]\n        result = log_parser(lines)\n        self.assertEqual(result['counts'], {'ERROR': 1, 'INFO': 1})\n        self.assertEqual(result['errors'], ['Connection failed'])\n        self.assertEqual(result['timestamp_range'], ('2024-01-15 10:30:45', '2024-01-15 10:31:00'))\n\n    def test_empty(self):\n        result = log_parser([])\n        self.assertEqual(result['counts'], {})\n        self.assertEqual(result['errors'], [])\n        self.assertIsNone(result['timestamp_range'])\n\n    def test_all_levels(self):\n        lines = [\n            '[2024-01-15 08:00:00] DEBUG: Debug msg',\n            '[2024-01-15 09:00:00] INFO: Info msg',\n            '[2024-01-15 10:00:00] WARNING: Warning msg',\n            '[2024-01-15 11:00:00] ERROR: Error msg',\n            '[2024-01-15 12:00:00] CRITICAL: Critical msg'\n        ]\n        result = log_parser(lines)\n        self.assertEqual(result['counts'], {'DEBUG': 1, 'INFO': 1, 'WARNING': 1, 'ERROR': 1, 'CRITICAL': 1})\n        self.assertEqual(result['errors'], ['Error msg', 'Critical msg'])\n        self.assertEqual(result['timestamp_range'], ('2024-01-15 08:00:00', '2024-01-15 12:00:00'))\n\n    def test_multiple_errors(self):\n        lines = [\n            '[2024-01-15 10:00:00] ERROR: First error',\n            '[2024-01-15 10:01:00] ERROR: Second error',\n            '[2024-01-15 10:02:00] CRITICAL: Critical failure'\n        ]\n        result = log_parser(lines)\n        self.assertEqual(result['counts'], {'ERROR': 2, 'CRITICAL': 1})\n        self.assertEqual(result['errors'], ['First error', 'Second error', 'Critical failure'])\n\n    def test_single_line(self):\n        lines = ['[2024-06-01 00:00:00] INFO: Server started']\n        result = log_parser(lines)\n        self.assertEqual(result['counts'], {'INFO': 1})\n        self.assertEqual(result['errors'], [])\n        self.assertEqual(result['timestamp_range'], ('2024-06-01 00:00:00', '2024-06-01 00:00:00'))\n\n    def test_unordered_timestamps(self):\n        lines = [\n            '[2024-01-15 12:00:00] INFO: Late',\n            '[2024-01-15 08:00:00] INFO: Early',\n            '[2024-01-15 10:00:00] INFO: Middle'\n        ]\n        result = log_parser(lines)\n        self.assertEqual(result['timestamp_range'], ('2024-01-15 08:00:00', '2024-01-15 12:00:00'))\n\n    def test_same_timestamp(self):\n        lines = [\n            '[2024-01-15 10:00:00] INFO: First',\n            '[2024-01-15 10:00:00] ERROR: Second'\n        ]\n        result = log_parser(lines)\n        self.assertEqual(result['timestamp_range'], ('2024-01-15 10:00:00', '2024-01-15 10:00:00'))\n\n    def test_message_with_colons(self):\n        lines = ['[2024-01-15 10:00:00] ERROR: URL: https://example.com failed']\n        result = log_parser(lines)\n        self.assertEqual(result['errors'], ['URL: https://example.com failed'])\n\n    def test_counts_accumulate(self):\n        lines = [\n            '[2024-01-15 10:00:00] INFO: msg1',\n            '[2024-01-15 10:01:00] INFO: msg2',\n            '[2024-01-15 10:02:00] INFO: msg3',\n            '[2024-01-15 10:03:00] WARNING: msg4',\n            '[2024-01-15 10:04:00] WARNING: msg5'\n        ]\n        result = log_parser(lines)\n        self.assertEqual(result['counts']['INFO'], 3)\n        self.assertEqual(result['counts']['WARNING'], 2)\n\n    def test_no_errors_in_output(self):\n        lines = [\n            '[2024-01-15 10:00:00] INFO: All good',\n            '[2024-01-15 10:01:00] DEBUG: Trace info'\n        ]\n        result = log_parser(lines)\n        self.assertEqual(result['errors'], [])",
    "tags": ["regex", "parsing", "log-analysis", "dictionary", "datetime"],
    "time_limit_seconds": 10
  }
]
