# 当所有大模型都能考 90 分，怎么找出真正的差距？

我们搭了一个 Python 编程能力评测框架（PyEval），拿 8 个主流模型跑了一圈，发现一个尴尬的问题：

**标准模式下，模型得分挤在 93%-100% 之间，几乎没有区分度。**

于是我们新增了两种评测模式，结果很有意思。分享一下。

---

## 背景：标准模式的天花板

PyEval 有 45 道 Python 编程题（easy/medium/hard），覆盖算法、OOP、并发等 10 个方向。标准模式就是给函数签名 + docstring，让模型写实现。

结果：

| 模型 | 标准模式通过率 |
|------|-------------|
| qwen3.5-plus | 100% |
| qwen3.5-flash | 100% |
| qwen3-max | 95.6% |
| deepseek-v3.2 | 95.6% |
| kimi-k2.5 | 93.3% |
| glm-4.7 | 93.3% |
| glm-5 | 68.9%† |

四个模型挤在 93-100%，这能说明什么？说明不了太多。

> †glm-5 的部分失败源于 API 超时/500 错误，非模型能力问题，实际分数应更高。

---

## 新模式一：Bug 修复

给模型一段有 bug 的代码 + bug 描述，让它修。15 道题覆盖经典 Python 陷阱：

- Easy：off-by-one、比较符写反、缺 return、变量拼错、条件反了
- Medium：浅拷贝、可变默认参数、错误异常类型、闭包捕获、整除 vs 浮点除
- Hard：线程竞态、生成器耗尽、装饰器丢 wraps、钻石继承 MRO、async 上下文管理器

**结果：**

| 模型 | Bugfix 通过率 |
|------|-------------|
| qwen3.5-plus | 15/15 (100%) |
| qwen3.5-flash | 15/15 (100%) |
| qwen3-max | 15/15 (100%) |
| deepseek-v3.2 | 15/15 (100%) |
| kimi-k2.5 | 15/15 (100%) |
| glm-4.7 | 14/15 (93.3%) |
| MiniMax-M2.5 | 13/15 (86.7%) |
| glm-5 | 10/15 (66.7%)† |

头部五个模型全满分——Bug 修复题对强模型来说确实不难，bug 位置和描述都给了，相当于开卷考试。但中部以下开始拉开差距。

> †glm-5 的 5 次失败中有 3 次是 API 超时/500 错误。

---

## 新模式二：多轮纠错（核心发现）

这才是真正拉开差距的模式。

**规则：**
- 第 1 次：标准调用，写对了算 1.0 分
- 第 1 次失败 → 把报错信息反馈给模型重试，写对了算 0.6 分
- 第 2 次还失败 → 再反馈再试，写对了算 0.3 分
- 3 次都没过 → 0 分

加权分 = Σ(尝试权重 × 难度权重) / Σ(难度权重)

这模拟的是真实开发场景：写完跑一下，报错了改，再跑，再改。

**结果：**

| 模型 | 首次通过 | 重试后通过 | 加权分 | 需纠错题数 |
|------|---------|-----------|--------|-----------|
| qwen3.5-plus | 97.8% | 100% | **99.1%** | 1 题 (att.2) |
| qwen3-max | 97.8% | 100% | **99.1%** | 1 题 (att.2) |
| kimi-k2.5 | 97.8% | 100% | **99.1%** | 1 题 (att.2) |
| qwen3.5-flash | 95.6% | 100% | **98.1%** | 2 题 (att.2) |
| deepseek-v3.2 | 95.6% | 100% | **98.1%** | 2 题 (att.2) |
| glm-4.7 | 80.0% | 93.3% | **86.0%** | 6 题 (att.2), 3 题失败 |
| MiniMax-M2.5 | 75.6% | 97.8% | **82.3%** | 5 题 (att.2), 5 题 (att.3), 1 题失败 |
| glm-5 | 60.0% | 77.8% | **62.2%**† | 6 题 (att.2), 2 题 (att.3), 10 题失败 |

> †glm-5 的多轮失败中大部分是 API 超时，非模型能力问题。

---

## 八模型全景对比

| 模型 | 标准模式 | Bugfix | 多轮加权 | 综合排名 |
|------|---------|--------|---------|---------|
| qwen3.5-plus | 100% | 100% | 99.1% | 1 |
| qwen3-max | 95.6% | 100% | 99.1% | 2 |
| kimi-k2.5 | 93.3% | 100% | 99.1% | 3 |
| qwen3.5-flash | 100% | 100% | 98.1% | 4 |
| deepseek-v3.2 | 95.6% | 100% | 98.1% | 5 |
| glm-4.7 | 93.3% | 93.3% | 86.0% | 6 |
| MiniMax-M2.5 | — | 86.7% | 82.3% | 7 |
| glm-5 | 68.9%† | 66.7%† | 62.2%† | 8† |

> †glm-5 受 API 稳定性影响较大，实际能力可能高于数据显示。

---

## 区分度对比：从 7pp 到 37pp

**标准模式区间：93.3% - 100%（差 7pp）**
**多轮加权区间：62.2% - 99.1%（差 37pp）**

即使排除受 API 影响的 glm-5：

**多轮加权区间（Top 7）：82.3% - 99.1%（差 17pp）**

区分度提升了 2.5 倍以上。

---

## 几个有趣的发现

**1. 第一梯队形成"三强"格局**

qwen3.5-plus、qwen3-max、kimi-k2.5 三个模型在多轮模式下都达到了 99.1% 的加权分，只各有 1 道题需要第 2 次尝试。它们的纠错能力几乎完美——给个报错就能立刻改对。

**2. kimi-k2.5 是隐藏的强者**

标准模式 93.3% 看起来一般，但多轮模式 99.1% 与 qwen3.5-plus 并列。这说明 kimi 的"一次性写对率"稍低，但纠错能力极强——给个报错信息就能立刻改对。在真实开发中，这种模型其实很好用。

**3. deepseek-v3.2 表现稳健**

标准模式 95.6%，Bugfix 满分，多轮 98.1%——在所有维度上都非常扎实。虽然没进第一梯队（差 1pp），但它是唯一一个在 Bugfix + 多轮都拿到高分的非 Qwen 系模型（kimi 除外）。

**4. qwen3.5-flash vs qwen3-max：意外的反转**

qwen3.5-flash 标准模式 100% 高于 qwen3-max 的 95.6%，但多轮加权却是 98.1% vs 99.1%，被 qwen3-max 反超。说明更大的模型在纠错能力上的优势能弥补首次通过率的差距。

**5. MiniMax-M2.5 的纠错依赖度最高**

首次通过率只有 75.6%，但通过 2-3 轮重试追到了 97.8%。有 5 道题要到第 3 次才写对。说明它"第一遍大概写个样子，靠报错信息逐步修正"的模式很明显。加权分因此被严重惩罚。

**6. "写对"和"改对"是两种不同能力**

标准模式只测"一次性写对"，但真实开发中，快速定位并修复错误的能力同样重要。多轮模式同时衡量了这两者，而且通过加权机制，一次写对的价值远高于多轮修改（1.0 vs 0.6 vs 0.3）。

---

## 评测方法

- 框架：PyEval（自研，纯 Python，无外部依赖）
- 标准题库：45 题 × 10 类别 × 3 难度
- Bugfix 题库：15 题 × 3 难度，覆盖经典 Python 陷阱
- 多轮纠错：最多 3 次尝试，递减加权（1.0 / 0.6 / 0.3）
- 所有模型通过阿里云百炼 DashScope API 调用，temperature=0
- 代码在隔离子进程中执行，通过 unittest 验证正确性

---

*如果你也在做模型评测，不妨试试多轮纠错模式——它能把 90 分的天花板撬开，看到更真实的能力分布。*
